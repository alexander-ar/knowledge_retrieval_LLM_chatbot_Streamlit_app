{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alexanderarefolov/Dropbox/Coding_Projects/knowledge_retrieval_LLM_chatbot_Streamlit_app/knowledge_retrieval_LLM_chatbot_Streamlit_app'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD_seller_signed.pdf                    app_scratch_book.ipynb\n",
      "Hatching_a_story.docx                   app_scratch_pinecone.ipynb\n",
      "LICENSE                                 final_signed_offer.pdf\n",
      "Purchase_and_sale_final.pdf             image2.png\n",
      "README.md                               image3.png\n",
      "RSM_packing_list.docx                   image5.png\n",
      "RSM_packing_list_2.docx                 image6.png\n",
      "Software_Engineering_Practices_TOP.txt  img.png\n",
      "app.py                                  requirements.txt\n",
      "app1.py                                 \u001b[34mtemp\u001b[m\u001b[m/\n",
      "app2.py                                 ~$tching_a_story.docx\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"Purchase_and_sale_final.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"final_signed_offer.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"CD_seller_signed.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"RSM_packing_list_2.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"Hatching_a_story.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexanderarefolov/Dropbox/Coding_Projects/knowledge_retrieval_LLM_chatbot_Streamlit_app/knowledge_retrieval_LLM_chatbot_Streamlit_app/.venv/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pinecone\n",
      "Version: 5.1.0\n",
      "Summary: Pinecone client and SDK\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: /Users/alexanderarefolov/Dropbox/Coding_Projects/knowledge_retrieval_LLM_chatbot_Streamlit_app/knowledge_retrieval_LLM_chatbot_Streamlit_app/.venv/lib/python3.11/site-packages\n",
      "Requires: certifi, pinecone-plugin-inference, pinecone-plugin-interface, tqdm, typing-extensions, urllib3\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pinecone'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m \u001b[38;5;66;03m###########################\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pinecone\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PodSpec    \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pinecone'"
     ]
    }
   ],
   "source": [
    "# Install all libraries by running in the terminal: pip install -r requirements.txt\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import os\n",
    "import tempfile\n",
    "import tiktoken\n",
    "import shutil ###########################\n",
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from pinecone import PodSpec    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch environmental variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "- Process input file\n",
    "- Load document\n",
    "- Calculate input costs\n",
    "- Split data in chunks\n",
    "- Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to process the input text file, remove empty lines and unneeded formatting marks\n",
    "def process_input_file(input_file_path):\n",
    "    '''\n",
    "    process_input_text() helper function takes the input file in txt, docx or pdf format\n",
    "    as an argument and removes empty lines and non-essential characters. The output is saved\n",
    "    in a temporary directory.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file_path (str): path to the input text file\n",
    "    \n",
    "    Returns:\n",
    "        processed temporary text file path saved in temp/\n",
    "    '''\n",
    "    # Create a temporary file in the same directory as the input file\n",
    "    temp_dir = os.path.join(os.path.dirname(input_file_path), \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok = True)\n",
    "\n",
    "    temp_file = tempfile.NamedTemporaryFile(mode = 'w', delete = False, dir = temp_dir, encoding = 'UTF-8')\n",
    "\n",
    "    try:\n",
    "        file_extension = os.path.splitext(input_file_path)[1].lower()\n",
    "\n",
    "        # Read the contents of the file based on its type\n",
    "        if file_extension == '.txt':\n",
    "            with open(input_file_path, 'r', encoding='UTF-8') as input_file:\n",
    "                lines = input_file.readlines()\n",
    "        elif file_extension == '.docx':\n",
    "            doc = Document(input_file_path)\n",
    "            lines = [p.text for p in doc.paragraphs]\n",
    "        elif file_extension == '.pdf':\n",
    "            with open(input_file_path, 'rb') as input_file:\n",
    "                reader = PyPDF2.PdfReader(input_file)\n",
    "                lines = []\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num] \n",
    "                    lines.append(page.extract_text())\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format: \" + file_extension)\n",
    "\n",
    "        # Remove empty lines and lines consisting only of '-' or '_'\n",
    "        non_empty_lines = [line.strip() for line in lines if line.strip() and not all(char in {'-', '_'} for char in line.strip())]\n",
    "\n",
    "        # Write processed text to the temporary file\n",
    "        temp_file.write('\\n'.join(non_empty_lines))\n",
    "    finally:\n",
    "        # Close the temporary file\n",
    "        temp_file.close()\n",
    "\n",
    "    # Get the path of the temporary file\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "    return temp_file_path\n",
    "\n",
    "\n",
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    '''\n",
    "    load_documents() is a helper function to load txt file\n",
    "    as langchain documents\n",
    "    \n",
    "    Parameters:\n",
    "        file (str): path to file\n",
    "    '''\n",
    "    try:\n",
    "        loader = TextLoader(file, encoding = 'UTF-8')\n",
    "    except:\n",
    "        print(\"TextLoader failed to load the text from load_documents function\")\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "# calculate embedding cost using tiktoken\n",
    "def calculate_input_embedding_cost(texts):\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    # print(f'Total Tokens: {total_tokens}')\n",
    "    # print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    return total_tokens, (total_tokens / 1000000) * 0.02\n",
    "\n",
    "\n",
    "# splitting data in chunks\n",
    "def chunk_data(data, chunk_size = 1024, chunk_overlap = 80):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    if len(chunks) == 0:\n",
    "        raise ValueError(\"Chunking failed - returned zero chunks!\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# # create embeddings using OpenAIEmbeddings() and save them in a Chroma vector store\n",
    "# def create_embeddings(chunks, persist_directory='./chroma_db'): #######################\n",
    "#     embeddings = OpenAIEmbeddings(\n",
    "#         model = os.getenv(\"TEXT_EMBEDDING_MODEL\"), \n",
    "#         dimensions=1536)  # 512 works as well\n",
    "#     # Create an in-memory Chroma vector store using the provided text chunks \n",
    "#     # and the embedding model \n",
    "#     vector_store = Chroma.from_documents(\n",
    "#         documents = chunks, \n",
    "#         embedding = embeddings,\n",
    "#         persist_directory = persist_directory)\n",
    "#     return vector_store\n",
    "\n",
    "def create_embeddings(index_name = \"real-estate-rag\", namespace = 'user1', chunks):\n",
    "\n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # loading from existing index\n",
    "    if index_name not in pc.list_indexes():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws', \n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "    # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "    vector_store = Pinecone.from_documents(\n",
    "        chunks, embeddings, index_name=index_name, namespace = namespace) \n",
    "    # processing the input documents, the chunks, geenrating the embeddings\n",
    "    # using the provided openAI embeddings instance, inserting the embedding intot he index and returning pincone vectoor store object.\n",
    "    print('Created the vector store')\n",
    "        \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the input document, remove temporary file and load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_file_path = process_input_file(input_file_path)\n",
    "if processed_text_file_path:\n",
    "    print(f\"Processed input file {input_file_path}\")\n",
    "\n",
    "data = load_document(processed_text_file_path)\n",
    "\n",
    "os.remove(processed_text_file_path)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Failed to load document: {input_file_path}\")\n",
    "else:\n",
    "    print(f\"Loaded the processed file {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text, calculate embedding cost and create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_data(data, chunk_size = 1024)\n",
    "print(f'Chunk size: 1024, Chunks: {len(chunks)}')\n",
    "\n",
    "tokens, embedding_cost = calculate_input_embedding_cost(chunks)\n",
    "print(f'Source document embedding cost: ${embedding_cost:.4f}')\n",
    "\n",
    "# creating the embeddings and returning the Chroma vector store\n",
    "vector_store = create_embeddings(chunks = chunks, namespace = 'user1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the document about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build messages\n",
    "system_template = r'''\n",
    "You are answering questions only concerning the provided content of the input document.  \n",
    "If you are asked a question that is not related to the document you response will be:\n",
    "'I can answer only the questions related to the source document!'.\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Answer questions only concerning the provided content of the input document.  \n",
    "If you are asked a question that is not related to the document you response will be:\n",
    "'I can answer only the questions related to the source document!'. \n",
    "Here is the user's question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "    ]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    model = os.getenv(\"OPENAI_DEPLOYMENT_NAME\"), \n",
    "    temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure vector store to act as a retriever (finding similar items, returning top k)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity', search_kwargs={'k': 3, 'namespace': 'user1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set up conversational retrieval chain\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = retriever,\n",
    "    memory = memory,\n",
    "    chain_type = 'stuff',\n",
    "    combine_docs_chain_kwargs = {'prompt': qa_prompt },\n",
    "    verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crc.invoke({'question': question})\n",
    "response = result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
