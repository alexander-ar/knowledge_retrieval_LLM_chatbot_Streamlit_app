{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alexanderarefolov/Dropbox/Coding_Projects/knowledge_retrieval_LLM_chatbot_Streamlit_app/knowledge_retrieval_LLM_chatbot_Streamlit_app'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CD_seller_signed.pdf                    app_scratch_book.ipynb\n",
      "Hatching_a_story.docx                   app_scratch_pinecone.ipynb\n",
      "LICENSE                                 final_signed_offer.pdf\n",
      "Purchase_and_sale_final.pdf             image2.png\n",
      "README.md                               image3.png\n",
      "RSM_packing_list.docx                   image5.png\n",
      "RSM_packing_list_2.docx                 image6.png\n",
      "Software_Engineering_Practices_TOP.txt  img.png\n",
      "app.py                                  requirements.txt\n",
      "app1.py                                 \u001b[34mtemp\u001b[m\u001b[m/\n",
      "app2.py                                 ~$tching_a_story.docx\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file_path = \"Purchase_and_sale_final.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file_path = \"final_signed_offer.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file_path = \"CD_seller_signed.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"RSM_packing_list_2.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_file_path = \"Hatching_a_story.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all libraries by running in the terminal: pip install -r requirements.txt\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import os\n",
    "import tempfile\n",
    "import tiktoken\n",
    "import shutil ###########################\n",
    "import pinecone\n",
    "from langchain_community.vectorstores import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch environmental variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "- Process input file\n",
    "- Load document\n",
    "- Calculate input costs\n",
    "- Split data in chunks\n",
    "- Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to process the input text file, remove empty lines and unneeded formatting marks\n",
    "def process_input_file(input_file_path):\n",
    "    '''\n",
    "    process_input_text() helper function takes the input file in txt, docx or pdf format\n",
    "    as an argument and removes empty lines and non-essential characters. The output is saved\n",
    "    in a temporary directory.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file_path (str): path to the input text file\n",
    "    \n",
    "    Returns:\n",
    "        processed temporary text file path saved in temp/\n",
    "    '''\n",
    "    # Create a temporary file in the same directory as the input file\n",
    "    temp_dir = os.path.join(os.path.dirname(input_file_path), \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok = True)\n",
    "\n",
    "    temp_file = tempfile.NamedTemporaryFile(mode = 'w', delete = False, dir = temp_dir, encoding = 'UTF-8')\n",
    "\n",
    "    try:\n",
    "        file_extension = os.path.splitext(input_file_path)[1].lower()\n",
    "\n",
    "        # Read the contents of the file based on its type\n",
    "        if file_extension == '.txt':\n",
    "            with open(input_file_path, 'r', encoding='UTF-8') as input_file:\n",
    "                lines = input_file.readlines()\n",
    "        elif file_extension == '.docx':\n",
    "            doc = Document(input_file_path)\n",
    "            lines = [p.text for p in doc.paragraphs]\n",
    "        elif file_extension == '.pdf':\n",
    "            with open(input_file_path, 'rb') as input_file:\n",
    "                reader = PyPDF2.PdfReader(input_file)\n",
    "                lines = []\n",
    "                for page_num in range(len(reader.pages)):\n",
    "                    page = reader.pages[page_num] \n",
    "                    lines.append(page.extract_text())\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format: \" + file_extension)\n",
    "\n",
    "        # Remove empty lines and lines consisting only of '-' or '_'\n",
    "        non_empty_lines = [line.strip() for line in lines if line.strip() and not all(char in {'-', '_'} for char in line.strip())]\n",
    "\n",
    "        # Write processed text to the temporary file\n",
    "        temp_file.write('\\n'.join(non_empty_lines))\n",
    "    finally:\n",
    "        # Close the temporary file\n",
    "        temp_file.close()\n",
    "\n",
    "    # Get the path of the temporary file\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "    return temp_file_path\n",
    "\n",
    "\n",
    "# loading PDF, DOCX and TXT files as LangChain Documents\n",
    "def load_document(file):\n",
    "    '''\n",
    "    load_documents() is a helper function to load txt file\n",
    "    as langchain documents\n",
    "    \n",
    "    Parameters:\n",
    "        file (str): path to file\n",
    "    '''\n",
    "    try:\n",
    "        loader = TextLoader(file, encoding = 'UTF-8')\n",
    "    except:\n",
    "        print(\"TextLoader failed to load the text from load_documents function\")\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data\n",
    "\n",
    "\n",
    "# calculate embedding cost using tiktoken\n",
    "def calculate_input_embedding_cost(texts):\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    # check prices here: https://openai.com/pricing\n",
    "    # print(f'Total Tokens: {total_tokens}')\n",
    "    # print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')\n",
    "    return total_tokens, (total_tokens / 1000000) * 0.02\n",
    "\n",
    "\n",
    "# splitting data in chunks\n",
    "def chunk_data(data, chunk_size = 1024, chunk_overlap = 80):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size, \n",
    "        chunk_overlap = chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    if len(chunks) == 0:\n",
    "        raise ValueError(\"Chunking failed - returned zero chunks!\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_embeddings(chunks, index_name = \"real-estate-rag\", namespace = 'user1'):\n",
    "\n",
    "    pc = pinecone.Pinecone()\n",
    "        \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
    "\n",
    "    # create index if does not exist yet\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud='aws', \n",
    "                region='us-east-1'\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
    "    # inserting the embeddings into the index and returning a new Pinecone vector store object. \n",
    "    vector_store = Pinecone.from_documents(\n",
    "        documents = chunks, \n",
    "        embedding = embeddings, \n",
    "        index_name = index_name, \n",
    "        namespace = namespace) \n",
    "    # processing the input documents, the chunks, geenrating the embeddings\n",
    "    # using the provided openAI embeddings instance, inserting the embedding intot he index and returning pincone vectoor store object.\n",
    "    print(f'Created vector store within {index_name} index and in {namespace} namespace')\n",
    "        \n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def delete_pinecode_index(index_name = 'all'):\n",
    "    pc = pinecone.Pinecone()\n",
    "\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print(\"Deleting all pinecone indexes...\")\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "    else:\n",
    "        print(f\"Deleting {index_name} pinecone index...\")\n",
    "        pc.delete_index(index_name)\n",
    "        print(f\"{index_name} pinecone index deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['real-estate-rag']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all pinecone indexes...\n"
     ]
    }
   ],
   "source": [
    "delete_pinecode_index(index_name = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the input document, remove temporary file and load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed input file RSM_packing_list_2.docx\n",
      "Loaded the processed file RSM_packing_list_2.docx\n"
     ]
    }
   ],
   "source": [
    "processed_text_file_path = process_input_file(input_file_path)\n",
    "if processed_text_file_path:\n",
    "    print(f\"Processed input file {input_file_path}\")\n",
    "\n",
    "data = load_document(processed_text_file_path)\n",
    "\n",
    "os.remove(processed_text_file_path)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Failed to load document: {input_file_path}\")\n",
    "else:\n",
    "    print(f\"Loaded the processed file {input_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the text, calculate embedding cost and create the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 1024, Chunks: 9\n",
      "Source document embedding cost: $0.0000\n",
      "Created the vector store\n",
      "<langchain_community.vectorstores.pinecone.Pinecone object at 0x12e76ea50>\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data, chunk_size = 1024)\n",
    "print(f'Chunk size: 1024, Chunks: {len(chunks)}')\n",
    "\n",
    "tokens, embedding_cost = calculate_input_embedding_cost(chunks)\n",
    "print(f'Source document embedding cost: ${embedding_cost:.4f}')\n",
    "\n",
    "# creating the embeddings and returning the Chroma vector store\n",
    "vector_store = create_embeddings(chunks = chunks, namespace = 'user1')\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the document about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build messages\n",
    "system_template = r'''\n",
    "You are answering questions only concerning the provided content of the input document.  \n",
    "If you are asked a question that is not related to the document you response will be:\n",
    "'I can answer only the questions related to the source document!'.\n",
    "---------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Answer questions only concerning the provided content of the input document.  \n",
    "If you are asked a question that is not related to the document you response will be:\n",
    "'I can answer only the questions related to the source document!'. \n",
    "Here is the user's question: ```{question}```\n",
    "'''\n",
    "\n",
    "messages= [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "    ]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    model = os.getenv(\"OPENAI_DEPLOYMENT_NAME\"), \n",
    "    temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure vector store to act as a retriever (finding similar items, returning top k)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity', search_kwargs={'k': 3, 'namespace': 'user1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set up conversational retrieval chain\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = retriever,\n",
    "    memory = memory,\n",
    "    chain_type = 'stuff',\n",
    "    combine_docs_chain_kwargs = {'prompt': qa_prompt },\n",
    "    verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crc.invoke({'question': question})\n",
    "response = result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The document is about the policies, procedures, and packing list for Camp Sunapee, directed by Olga Pristin, Ann Baranov, and Ellen Arbeznik. It emphasizes the importance of compliance with camp rules to maintain a safe and enjoyable environment. The document includes detailed instructions on what to pack, health guidelines, and a strict policy against bringing food into the cabins to avoid attracting pests and creating litter. It also mentions the necessity of labeling all belongings and the submission of health forms by a specific deadline.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now reading another document and creating another vector db using a different namespace within the same pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"Hatching_a_story.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed input file Hatching_a_story.docx\n",
      "Loaded the processed file Hatching_a_story.docx\n"
     ]
    }
   ],
   "source": [
    "processed_text_file_path = process_input_file(input_file_path)\n",
    "if processed_text_file_path:\n",
    "    print(f\"Processed input file {input_file_path}\")\n",
    "\n",
    "data = load_document(processed_text_file_path)\n",
    "\n",
    "os.remove(processed_text_file_path)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Failed to load document: {input_file_path}\")\n",
    "else:\n",
    "    print(f\"Loaded the processed file {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size: 1024, Chunks: 1\n",
      "Source document embedding cost: $0.0000\n",
      "Created the vector store\n",
      "<langchain_community.vectorstores.pinecone.Pinecone object at 0x12e7679d0>\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data, chunk_size = 1024)\n",
    "print(f'Chunk size: 1024, Chunks: {len(chunks)}')\n",
    "\n",
    "tokens, embedding_cost = calculate_input_embedding_cost(chunks)\n",
    "print(f'Source document embedding cost: ${embedding_cost:.4f}')\n",
    "\n",
    "# creating the embeddings and returning the Chroma vector store\n",
    "vector_store = create_embeddings(chunks = chunks, namespace = 'user2') # used different namespace\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/alexanderarefolov/Dropbox/Coding_Projects/knowledge_retrieval_LLM_chatbot_Streamlit_app/knowledge_retrieval_LLM_chatbot_Streamlit_app/temp/tmpef0oq761', 'text': \"Hatching a story\\nThe egg was a light blue the size of a football and had red and yellow spots and began to crack and now it popped a baby dragon! it coughed and I'll pop the little flame I took it home and secretly fed it he got too big so I had to hide it in my backyard after a few years passed grow goo got too big and turned into a fully sized dragon then robbers broke into my house my mom and dad were away on the theater play so I was home alone I ran into my backyard robbers after me then my dragon stepped out and used his breath and turned the robbers into ashes after that I found out they were famous Roberts and earned the Nobel Prize\"}, page_content=\"Hatching a story\\nThe egg was a light blue the size of a football and had red and yellow spots and began to crack and now it popped a baby dragon! it coughed and I'll pop the little flame I took it home and secretly fed it he got too big so I had to hide it in my backyard after a few years passed grow goo got too big and turned into a fully sized dragon then robbers broke into my house my mom and dad were away on the theater play so I was home alone I ran into my backyard robbers after me then my dragon stepped out and used his breath and turned the robbers into ashes after that I found out they were famous Roberts and earned the Nobel Prize\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the document about?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\"),  \n",
    "    model = os.getenv(\"OPENAI_DEPLOYMENT_NAME\"), \n",
    "    temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure vector store to act as a retriever (finding similar items, returning top k)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type='similarity', search_kwargs={'k': 3, 'namespace': 'user2'}) # use the second namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a memory buffer to track the conversation\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Set up conversational retrieval chain\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = retriever,\n",
    "    memory = memory,\n",
    "    chain_type = 'stuff',\n",
    "    combine_docs_chain_kwargs = {'prompt': qa_prompt },\n",
    "    verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crc.invoke({'question': question})\n",
    "response = result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The document is about a person who finds a large, uniquely colored egg that hatches into a baby dragon. The person secretly raises the dragon at home, and as it grows larger, hides it in the backyard. When robbers break into the house while the person's parents are away, the dragon protects the person by using its fiery breath to turn the robbers into ashes. The robbers turn out to be famous, and the person earns a Nobel Prize as a result of the incident.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
